[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 11197 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
[main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8888 (http)
[main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
[main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
[main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
[main] INFO  o.a.c.c.C.[.[localhost].[/pgf/api] - Initializing Spring embedded WebApplicationContext
[main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1165 ms
[main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[main] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843436703
[kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3010 with epoch 0
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.p.ProducerNoKeysStickyP - Message sent! Partition:2  Offset:96 Timestamp:1709843437081
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.p.ProducerNoKeysStickyP - Message sent! Partition:2  Offset:97 Timestamp:1709843438096
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:74 Timestamp:1709843439105]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:111 Timestamp:1709843440107]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:98 Timestamp:1709843441107]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:112 Timestamp:1709843442108]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:75 Timestamp:1709843443109]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:113 Timestamp:1709843444110]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:99 Timestamp:1709843445111]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:114 Timestamp:1709843446111]
[main] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
[main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
[main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843447568
[main] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843447583
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[main] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.8 seconds (process running for 14.612)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-07e0ae09-5781-4887-84f3-c63d9304cf11
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-b160d1ab-7def-419d-9813-146a8aa253ea
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10018, memberId='consumer-secondT-consumer-2-b160d1ab-7def-419d-9813-146a8aa253ea', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10018, memberId='consumer-secondT-consumer-1-07e0ae09-5781-4887-84f3-c63d9304cf11', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Finished assignment for group at generation 10018: {consumer-secondT-consumer-2-b160d1ab-7def-419d-9813-146a8aa253ea=Assignment(partitions=[second_topic-2]), consumer-secondT-consumer-1-07e0ae09-5781-4887-84f3-c63d9304cf11=Assignment(partitions=[second_topic-0, second_topic-1])}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10018, memberId='consumer-secondT-consumer-1-07e0ae09-5781-4887-84f3-c63d9304cf11', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10018, memberId='consumer-secondT-consumer-2-b160d1ab-7def-419d-9813-146a8aa253ea', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=74, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=111, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:74] - [0:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:96] - Sticky Value:0 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:97] - Sticky Value:1 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:75] - [1:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:98] - [0:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:111] - [0:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:99] - [1:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:112] - [0:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:113] - [1:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:114] - [1:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-07e0ae09-5781-4887-84f3-c63d9304cf11 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-b160d1ab-7def-419d-9813-146a8aa253ea sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
[SpringApplicationShutdownHook] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
[SpringApplicationShutdownHook] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
[SpringApplicationShutdownHook] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[SpringApplicationShutdownHook] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
[SpringApplicationShutdownHook] INFO  o.a.kafka.common.utils.AppInfoParser - App info kafka.producer for producer-1 unregistered
[main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.IllegalStateException: java.lang.IllegalStateException: Logback configuration error detected: 
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for conversion word [wEx]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - [wEx] is not a valid conversion word
	at org.springframework.boot.context.logging.LoggingApplicationListener.initializeSystem(LoggingApplicationListener.java:347)
	at org.springframework.boot.context.logging.LoggingApplicationListener.initialize(LoggingApplicationListener.java:298)
	at org.springframework.boot.context.logging.LoggingApplicationListener.onApplicationEnvironmentPreparedEvent(LoggingApplicationListener.java:246)
	at org.springframework.boot.context.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:223)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:178)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:171)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:149)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:137)
	at org.springframework.boot.context.event.EventPublishingRunListener.multicastInitialEvent(EventPublishingRunListener.java:136)
	at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:81)
	at org.springframework.boot.SpringApplicationRunListeners.lambda$environmentPrepared$2(SpringApplicationRunListeners.java:64)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at org.springframework.boot.SpringApplicationRunListeners.doWithListeners(SpringApplicationRunListeners.java:118)
	at org.springframework.boot.SpringApplicationRunListeners.doWithListeners(SpringApplicationRunListeners.java:112)
	at org.springframework.boot.SpringApplicationRunListeners.environmentPrepared(SpringApplicationRunListeners.java:63)
	at org.springframework.boot.SpringApplication.prepareEnvironment(SpringApplication.java:366)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:318)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at org.kkpa.playgroundsb.PlaygroundFrameworksSpringbootApplication.main(PlaygroundFrameworksSpringbootApplication.java:10)
Caused by: java.lang.IllegalStateException: Logback configuration error detected: 
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for conversion word [wEx]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - [wEx] is not a valid conversion word
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.reportConfigurationErrorsIfNecessary(LogbackLoggingSystem.java:277)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.loadConfiguration(LogbackLoggingSystem.java:255)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.reinitialize(LogbackLoggingSystem.java:334)
	at org.springframework.boot.logging.AbstractLoggingSystem.initializeWithConventions(AbstractLoggingSystem.java:74)
	at org.springframework.boot.logging.AbstractLoggingSystem.initialize(AbstractLoggingSystem.java:61)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.initialize(LogbackLoggingSystem.java:189)
	at org.springframework.boot.context.logging.LoggingApplicationListener.initializeSystem(LoggingApplicationListener.java:332)
	... 19 common frames omitted
[main] ERROR o.s.boot.SpringApplication - Application run failed
java.lang.IllegalStateException: java.lang.IllegalStateException: Logback configuration error detected: 
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for conversion word [wEx]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - [wEx] is not a valid conversion word
	at org.springframework.boot.context.logging.LoggingApplicationListener.initializeSystem(LoggingApplicationListener.java:347)
	at org.springframework.boot.context.logging.LoggingApplicationListener.initialize(LoggingApplicationListener.java:298)
	at org.springframework.boot.context.logging.LoggingApplicationListener.onApplicationEnvironmentPreparedEvent(LoggingApplicationListener.java:246)
	at org.springframework.boot.context.logging.LoggingApplicationListener.onApplicationEvent(LoggingApplicationListener.java:223)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.doInvokeListener(SimpleApplicationEventMulticaster.java:178)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.invokeListener(SimpleApplicationEventMulticaster.java:171)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:149)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:137)
	at org.springframework.boot.context.event.EventPublishingRunListener.multicastInitialEvent(EventPublishingRunListener.java:136)
	at org.springframework.boot.context.event.EventPublishingRunListener.environmentPrepared(EventPublishingRunListener.java:81)
	at org.springframework.boot.SpringApplicationRunListeners.lambda$environmentPrepared$2(SpringApplicationRunListeners.java:64)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at org.springframework.boot.SpringApplicationRunListeners.doWithListeners(SpringApplicationRunListeners.java:118)
	at org.springframework.boot.SpringApplicationRunListeners.doWithListeners(SpringApplicationRunListeners.java:112)
	at org.springframework.boot.SpringApplicationRunListeners.environmentPrepared(SpringApplicationRunListeners.java:63)
	at org.springframework.boot.SpringApplication.prepareEnvironment(SpringApplication.java:366)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:318)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1342)
	at org.springframework.boot.SpringApplication.run(SpringApplication.java:1331)
	at org.kkpa.playgroundsb.PlaygroundFrameworksSpringbootApplication.main(PlaygroundFrameworksSpringbootApplication.java:10)
Caused by: java.lang.IllegalStateException: Logback configuration error detected: 
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for composite conversion word [clr]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - Failed to create converter for [%clr] keyword
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - There is no conversion class registered for conversion word [wEx]
ERROR in ch.qos.logback.core.pattern.parser.Compiler@52d239ba - [wEx] is not a valid conversion word
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.reportConfigurationErrorsIfNecessary(LogbackLoggingSystem.java:277)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.loadConfiguration(LogbackLoggingSystem.java:255)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.reinitialize(LogbackLoggingSystem.java:334)
	at org.springframework.boot.logging.AbstractLoggingSystem.initializeWithConventions(AbstractLoggingSystem.java:74)
	at org.springframework.boot.logging.AbstractLoggingSystem.initialize(AbstractLoggingSystem.java:61)
	at org.springframework.boot.logging.logback.LogbackLoggingSystem.initialize(LogbackLoggingSystem.java:189)
	at org.springframework.boot.context.logging.LoggingApplicationListener.initializeSystem(LoggingApplicationListener.java:332)
	... 19 common frames omitted
[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 11765 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
[main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat initialized with port 8888 (http)
[main] INFO  o.a.coyote.http11.Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
[main] INFO  o.a.catalina.core.StandardService - Starting service [Tomcat]
[main] INFO  o.a.catalina.core.StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
[main] INFO  o.a.c.c.C.[.[localhost].[/pgf/api] - Initializing Spring embedded WebApplicationContext
[main] INFO  o.s.b.w.s.c.ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1167 ms
[main] INFO  o.a.k.c.producer.ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[main] INFO  o.a.k.clients.producer.KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843576395
[kafka-producer-network-thread | producer-1] INFO  org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[kafka-producer-network-thread | producer-1] INFO  o.a.k.c.p.i.TransactionManager - [Producer clientId=producer-1] ProducerId set to 3011 with epoch 0
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.p.ProducerNoKeysStickyP - Message sent! Partition:0  Offset:115 Timestamp:1709843576764
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.p.ProducerNoKeysStickyP - Message sent! Partition:0  Offset:116 Timestamp:1709843577780
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:76 Timestamp:1709843578788]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:117 Timestamp:1709843579790]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:100 Timestamp:1709843580791]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:118 Timestamp:1709843581791]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:77 Timestamp:1709843582792]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:119 Timestamp:1709843583793]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:101 Timestamp:1709843584794]
[kafka-producer-network-thread | producer-1] INFO  o.k.p.k.producers.ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:120 Timestamp:1709843585794]
[main] INFO  o.a.coyote.http11.Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
[main] INFO  o.s.b.w.e.tomcat.TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
[main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843587273
[main] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[main] INFO  o.a.k.c.consumer.ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka version: 3.6.0
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  o.a.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1709843587290
[main] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  org.apache.kafka.clients.Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[main] INFO  o.k.p.PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.781 seconds (process running for 14.734)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-682809ac-f48c-407c-8af7-bc6cb7307a0a
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-58d551ec-e3dd-45fb-92ff-913c156751a9
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10020, memberId='consumer-secondT-consumer-2-682809ac-f48c-407c-8af7-bc6cb7307a0a', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10020, memberId='consumer-secondT-consumer-1-58d551ec-e3dd-45fb-92ff-913c156751a9', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Finished assignment for group at generation 10020: {consumer-secondT-consumer-1-58d551ec-e3dd-45fb-92ff-913c156751a9=Assignment(partitions=[second_topic-0, second_topic-1]), consumer-secondT-consumer-2-682809ac-f48c-407c-8af7-bc6cb7307a0a=Assignment(partitions=[second_topic-2])}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10020, memberId='consumer-secondT-consumer-2-682809ac-f48c-407c-8af7-bc6cb7307a0a', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10020, memberId='consumer-secondT-consumer-1-58d551ec-e3dd-45fb-92ff-913c156751a9', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=100, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=76, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=115, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:76] - [0:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:100] - [0:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:77] - [1:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:101] - [1:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:115] - Sticky Value:0 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:116] - Sticky Value:1 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:117] - [0:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:118] - [0:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:119] - [1:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.k.p.k.c.SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:120] - [1:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.l.KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-682809ac-f48c-407c-8af7-bc6cb7307a0a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-58d551ec-e3dd-45fb-92ff-913c156751a9 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.s.k.c.DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.k.c.c.i.ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics scheduler closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  o.a.kafka.common.metrics.Metrics - Metrics reporters closed
[main] INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 12114 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
[main] INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
[main] INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
[main] INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
[main] INFO  StandardService - Starting service [Tomcat]
[main] INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
[main] INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
[main] INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1343 ms
[main] INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

[main] INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
[main] INFO  AppInfoParser - Kafka version: 3.6.0
[main] INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  AppInfoParser - Kafka startTimeMs: 1709843907428
[kafka-producer-network-thread | producer-1] INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[kafka-producer-network-thread | producer-1] INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3012 with epoch 0
[kafka-producer-network-thread | producer-1] INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:121 Timestamp:1709843907843
[kafka-producer-network-thread | producer-1] INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:122 Timestamp:1709843908859
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:78 Timestamp:1709843909863]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:123 Timestamp:1709843910864]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:102 Timestamp:1709843911865]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:124 Timestamp:1709843912865]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:79 Timestamp:1709843913866]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:125 Timestamp:1709843914867]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:103 Timestamp:1709843915868]
[kafka-producer-network-thread | producer-1] INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:126 Timestamp:1709843916869]
[main] INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
[main] INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
[main] INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  AppInfoParser - Kafka version: 3.6.0
[main] INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  AppInfoParser - Kafka startTimeMs: 1709843918318
[main] INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[main] INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

[main] INFO  AppInfoParser - Kafka version: 3.6.0
[main] INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
[main] INFO  AppInfoParser - Kafka startTimeMs: 1709843918332
[main] INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[main] INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 14.011 seconds (process running for 14.897)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-b8db23e4-1315-403b-9e72-fc92c7c1fb0a
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-f21e8e25-fc89-4725-9f4d-1452598b137c
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10022, memberId='consumer-secondT-consumer-2-f21e8e25-fc89-4725-9f4d-1452598b137c', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10022, memberId='consumer-secondT-consumer-1-b8db23e4-1315-403b-9e72-fc92c7c1fb0a', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Finished assignment for group at generation 10022: {consumer-secondT-consumer-1-b8db23e4-1315-403b-9e72-fc92c7c1fb0a=Assignment(partitions=[second_topic-0, second_topic-1]), consumer-secondT-consumer-2-f21e8e25-fc89-4725-9f4d-1452598b137c=Assignment(partitions=[second_topic-2])}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10022, memberId='consumer-secondT-consumer-2-f21e8e25-fc89-4725-9f4d-1452598b137c', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10022, memberId='consumer-secondT-consumer-1-b8db23e4-1315-403b-9e72-fc92c7c1fb0a', protocol='range'}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=102, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=78, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=121, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:78] - [0:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:102] - [0:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:103] - [1:2] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:79] - [1:0] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:121] - Sticky Value:0 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:122] - Sticky Value:1 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:123] - [0:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:124] - [0:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:125] - [1:1] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:126] - [1:3] 
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-b8db23e4-1315-403b-9e72-fc92c7c1fb0a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-f21e8e25-fc89-4725-9f4d-1452598b137c sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  Metrics - Metrics scheduler closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  Metrics - Metrics scheduler closed
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  Metrics - Metrics reporters closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  Metrics - Metrics reporters closed
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
[org.springframework.kafka.KafkaListenerEndpointContainer#0-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
[org.springframework.kafka.KafkaListenerEndpointContainer#1-0-C-1] INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
[SpringApplicationShutdownHook] INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
[SpringApplicationShutdownHook] INFO  Metrics - Metrics scheduler closed
[SpringApplicationShutdownHook] INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[SpringApplicationShutdownHook] INFO  Metrics - Metrics reporters closed
[SpringApplicationShutdownHook] INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 12374 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1127 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709843969235
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3013 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:104 Timestamp:1709843969593
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:105 Timestamp:1709843970608
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:80 Timestamp:1709843971615]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:127 Timestamp:1709843972617]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:106 Timestamp:1709843973618]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:128 Timestamp:1709843974618]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:81 Timestamp:1709843975619]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:129 Timestamp:1709843976620]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:107 Timestamp:1709843977621]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:130 Timestamp:1709843978622]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709843980043
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709843980057
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.653 seconds (process running for 14.499)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-fa5c4f81-133c-4116-8add-39f77295f0cd
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-6daf2a9c-eaf1-4cd6-aa7a-71f967749393
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10024, memberId='consumer-secondT-consumer-2-6daf2a9c-eaf1-4cd6-aa7a-71f967749393', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10024, memberId='consumer-secondT-consumer-1-fa5c4f81-133c-4116-8add-39f77295f0cd', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Finished assignment for group at generation 10024: {consumer-secondT-consumer-2-6daf2a9c-eaf1-4cd6-aa7a-71f967749393=Assignment(partitions=[second_topic-2]), consumer-secondT-consumer-1-fa5c4f81-133c-4116-8add-39f77295f0cd=Assignment(partitions=[second_topic-0, second_topic-1])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10024, memberId='consumer-secondT-consumer-1-fa5c4f81-133c-4116-8add-39f77295f0cd', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10024, memberId='consumer-secondT-consumer-2-6daf2a9c-eaf1-4cd6-aa7a-71f967749393', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=80, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=104, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=127, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:104] - Sticky Value:0 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:80] - [0:0] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:81] - [1:0] 
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:105] - Sticky Value:1 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:127] - [0:1] 
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:106] - [0:2] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:128] - [0:3] 
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:107] - [1:2] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:129] - [1:1] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:130] - [1:3] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-6daf2a9c-eaf1-4cd6-aa7a-71f967749393 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-fa5c4f81-133c-4116-8add-39f77295f0cd sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 13841 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1386 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844312373
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3014 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:1  Offset:82 Timestamp:1709844312798
INFO  ProducerNoKeysStickyP - Message sent! Partition:1  Offset:83 Timestamp:1709844313814
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:84 Timestamp:1709844314819]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:131 Timestamp:1709844315820]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:108 Timestamp:1709844316820]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:132 Timestamp:1709844317821]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:85 Timestamp:1709844318821]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:133 Timestamp:1709844319822]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:109 Timestamp:1709844320822]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:134 Timestamp:1709844321823]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844323344
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844323375
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 14.225 seconds (process running for 15.208)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-9232159e-a8a4-442e-8f21-7f818b93d82d
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-10f1b1bb-d8e8-4343-aac8-db07134d2dee
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10026, memberId='consumer-secondT-consumer-2-9232159e-a8a4-442e-8f21-7f818b93d82d', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10026, memberId='consumer-secondT-consumer-1-10f1b1bb-d8e8-4343-aac8-db07134d2dee', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Finished assignment for group at generation 10026: {consumer-secondT-consumer-1-10f1b1bb-d8e8-4343-aac8-db07134d2dee=Assignment(partitions=[second_topic-0, second_topic-1]), consumer-secondT-consumer-2-9232159e-a8a4-442e-8f21-7f818b93d82d=Assignment(partitions=[second_topic-2])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10026, memberId='consumer-secondT-consumer-2-9232159e-a8a4-442e-8f21-7f818b93d82d', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10026, memberId='consumer-secondT-consumer-1-10f1b1bb-d8e8-4343-aac8-db07134d2dee', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=108, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=82, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=131, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:82] - Sticky Value:0 
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:108] - [0:2] 
INFO  SecondTopicConsumer2 - Consumer 2 [Partition:2:Offset:109] - [1:2] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:83] - Sticky Value:1 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:84] - [0:0] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:1:Offset:85] - [1:0] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:131] - [0:1] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:132] - [0:3] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:133] - [1:1] 
INFO  SecondTopicConsumer1 - Consumer 1 [Partition:0:Offset:134] - [1:3] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-9232159e-a8a4-442e-8f21-7f818b93d82d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-10f1b1bb-d8e8-4343-aac8-db07134d2dee sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 14166 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1190 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844465010
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3015 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:135 Timestamp:1709844465424
INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:136 Timestamp:1709844466441
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:86 Timestamp:1709844467445]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:137 Timestamp:1709844468446]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:110 Timestamp:1709844469447]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:138 Timestamp:1709844470447]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:87 Timestamp:1709844471448]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:139 Timestamp:1709844472448]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:111 Timestamp:1709844473449]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:140 Timestamp:1709844474450]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844475963
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844475980
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.938 seconds (process running for 14.895)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-aa73074d-0692-422e-b3ff-d02ff0998460
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-a7e3e968-b0d4-46dd-a056-b87a08210c1f
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10028, memberId='consumer-secondT-consumer-1-a7e3e968-b0d4-46dd-a056-b87a08210c1f', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10028, memberId='consumer-secondT-consumer-2-aa73074d-0692-422e-b3ff-d02ff0998460', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Finished assignment for group at generation 10028: {consumer-secondT-consumer-2-aa73074d-0692-422e-b3ff-d02ff0998460=Assignment(partitions=[second_topic-2]), consumer-secondT-consumer-1-a7e3e968-b0d4-46dd-a056-b87a08210c1f=Assignment(partitions=[second_topic-0, second_topic-1])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10028, memberId='consumer-secondT-consumer-1-a7e3e968-b0d4-46dd-a056-b87a08210c1f', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10028, memberId='consumer-secondT-consumer-2-aa73074d-0692-422e-b3ff-d02ff0998460', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=110, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=86, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=135, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
INFO  SecondTopicConsumer2 - Consumer 2 [Key:id_2_Partition:2_Offset:110] - Value:[0:2] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_0_Partition:1_Offset:86] - Value:[0:0] 
INFO  SecondTopicConsumer2 - Consumer 2 [Key:id_2_Partition:2_Offset:111] - Value:[1:2] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_0_Partition:1_Offset:87] - Value:[1:0] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:null_Partition:0_Offset:135] - Value:Sticky Value:0 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:null_Partition:0_Offset:136] - Value:Sticky Value:1 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_1_Partition:0_Offset:137] - Value:[0:1] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_3_Partition:0_Offset:138] - Value:[0:3] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_1_Partition:0_Offset:139] - Value:[1:1] 
INFO  SecondTopicConsumer1 - Consumer 1 [Key:id_3_Partition:0_Offset:140] - Value:[1:3] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-aa73074d-0692-422e-b3ff-d02ff0998460 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-a7e3e968-b0d4-46dd-a056-b87a08210c1f sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 14498 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1161 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844683097
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3016 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:112 Timestamp:1709844683447
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:113 Timestamp:1709844684462
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:88 Timestamp:1709844685467]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:141 Timestamp:1709844686468]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:114 Timestamp:1709844687469]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:142 Timestamp:1709844688470]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:89 Timestamp:1709844689470]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:143 Timestamp:1709844690471]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:115 Timestamp:1709844691471]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:144 Timestamp:1709844692472]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844693958
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844693973
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.76 seconds (process running for 14.539)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-39942fab-3efe-466b-852e-61351b0c5b71
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-75c072b0-9b42-4b93-af37-61b7c917efee
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10030, memberId='consumer-secondT-consumer-1-75c072b0-9b42-4b93-af37-61b7c917efee', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10030, memberId='consumer-secondT-consumer-2-39942fab-3efe-466b-852e-61351b0c5b71', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Finished assignment for group at generation 10030: {consumer-secondT-consumer-2-39942fab-3efe-466b-852e-61351b0c5b71=Assignment(partitions=[second_topic-2]), consumer-secondT-consumer-1-75c072b0-9b42-4b93-af37-61b7c917efee=Assignment(partitions=[second_topic-0, second_topic-1])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10030, memberId='consumer-secondT-consumer-2-39942fab-3efe-466b-852e-61351b0c5b71', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10030, memberId='consumer-secondT-consumer-1-75c072b0-9b42-4b93-af37-61b7c917efee', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0, second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=88, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=112, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=141, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0, second_topic-1]
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:88] - Value:[0:0] 
INFO  SecondTopicConsumer2 - Consumer 2 [Key:null_Partition:2_Offset:112] - Value:Sticky Value:0 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:89] - Value:[1:0] 
INFO  SecondTopicConsumer2 - Consumer 2 [Key:null_Partition:2_Offset:113] - Value:Sticky Value:1 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_1_Partition:0_Offset:141] - Value:[0:1] 
INFO  SecondTopicConsumer2 - Consumer 2 [Key:id_2_Partition:2_Offset:114] - Value:[0:2] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_3_Partition:0_Offset:142] - Value:[0:3] 
INFO  SecondTopicConsumer2 - Consumer 2 [Key:id_2_Partition:2_Offset:115] - Value:[1:2] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_1_Partition:0_Offset:143] - Value:[1:1] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_3_Partition:0_Offset:144] - Value:[1:3] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0, second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0, second_topic-1]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-75c072b0-9b42-4b93-af37-61b7c917efee sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-39942fab-3efe-466b-852e-61351b0c5b71 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 14814 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1116 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844836257
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3017 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:1  Offset:90 Timestamp:1709844836618
INFO  ProducerNoKeysStickyP - Message sent! Partition:1  Offset:91 Timestamp:1709844837631
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:92 Timestamp:1709844838637]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:145 Timestamp:1709844839639]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:116 Timestamp:1709844840639]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:146 Timestamp:1709844841640]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:93 Timestamp:1709844842641]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:147 Timestamp:1709844843642]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:117 Timestamp:1709844844643]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:148 Timestamp:1709844845644]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844847086
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844847105
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-secondT-consumer-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = secondT-consumer
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844847115
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-2-422f6ecc-d480-4716-af04-f07aacb292a6
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-1-3bcf243c-d339-4293-8ae2-f47f213360df
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.69 seconds (process running for 14.582)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Request joining group due to: need to re-join with the given member-id: consumer-secondT-consumer-3-203d5f70-8c96-4b29-a2fb-5420cf50b6eb
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10032, memberId='consumer-secondT-consumer-1-3bcf243c-d339-4293-8ae2-f47f213360df', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10032, memberId='consumer-secondT-consumer-2-422f6ecc-d480-4716-af04-f07aacb292a6', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Successfully joined group with generation Generation{generationId=10032, memberId='consumer-secondT-consumer-3-203d5f70-8c96-4b29-a2fb-5420cf50b6eb', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Finished assignment for group at generation 10032: {consumer-secondT-consumer-2-422f6ecc-d480-4716-af04-f07aacb292a6=Assignment(partitions=[second_topic-1]), consumer-secondT-consumer-1-3bcf243c-d339-4293-8ae2-f47f213360df=Assignment(partitions=[second_topic-0]), consumer-secondT-consumer-3-203d5f70-8c96-4b29-a2fb-5420cf50b6eb=Assignment(partitions=[second_topic-2])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10032, memberId='consumer-secondT-consumer-3-203d5f70-8c96-4b29-a2fb-5420cf50b6eb', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10032, memberId='consumer-secondT-consumer-2-422f6ecc-d480-4716-af04-f07aacb292a6', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Successfully synced group in generation Generation{generationId=10032, memberId='consumer-secondT-consumer-1-3bcf243c-d339-4293-8ae2-f47f213360df', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Notifying assignor about the new Assignment(partitions=[second_topic-0])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Adding newly assigned partitions: second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=116, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=90, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=145, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-0]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions assigned: [second_topic-1]
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:id_2_Partition:2_Offset:116] - Value:[0:2] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_1_Partition:0_Offset:145] - Value:[0:1] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:null_Partition:1_Offset:90] - Value:Sticky Value:0 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_3_Partition:0_Offset:146] - Value:[0:3] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:null_Partition:1_Offset:91] - Value:Sticky Value:1 
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:id_2_Partition:2_Offset:117] - Value:[1:2] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_1_Partition:0_Offset:147] - Value:[1:1] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:92] - Value:[0:0] 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:93] - Value:[1:0] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_3_Partition:0_Offset:148] - Value:[1:3] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Revoke previously assigned partitions second_topic-0
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-2]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-1]
INFO  KafkaMessageListenerContainer - secondT-consumer: partitions revoked: [second_topic-0]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Member consumer-secondT-consumer-2-422f6ecc-d480-4716-af04-f07aacb292a6 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Member consumer-secondT-consumer-1-3bcf243c-d339-4293-8ae2-f47f213360df sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Member consumer-secondT-consumer-3-203d5f70-8c96-4b29-a2fb-5420cf50b6eb sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-2, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-3, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-secondT-consumer-1, groupId=secondT-consumer] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-3 unregistered
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-1 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  AppInfoParser - App info kafka.consumer for consumer-secondT-consumer-2 unregistered
INFO  KafkaMessageListenerContainer - secondT-consumer: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 15115 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1175 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844948843
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3018 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:149 Timestamp:1709844949183
INFO  ProducerNoKeysStickyP - Message sent! Partition:0  Offset:150 Timestamp:1709844950198
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:94 Timestamp:1709844951206]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:151 Timestamp:1709844952207]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:118 Timestamp:1709844953208]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:152 Timestamp:1709844954208]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:95 Timestamp:1709844955209]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:153 Timestamp:1709844956210]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:119 Timestamp:1709844957211]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:154 Timestamp:1709844958212]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844959670
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-1, groupId=groupST] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844959687
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-2, groupId=groupST] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-groupST-1, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] (Re-)joining group
INFO  Metadata - [Consumer clientId=consumer-groupST-2, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] (Re-)joining group
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709844959695
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-3, groupId=groupST] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-groupST-3, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-2-faba98d1-073f-4166-aaee-518537b566df
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-1-ada12034-52f3-41a8-9b62-d3efd648bc32
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-3-11eb11d4-3be6-4adf-97b7-83f55437b38d
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] (Re-)joining group
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 13.726 seconds (process running for 14.536)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Successfully joined group with generation Generation{generationId=1, memberId='consumer-groupST-1-ada12034-52f3-41a8-9b62-d3efd648bc32', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Successfully joined group with generation Generation{generationId=1, memberId='consumer-groupST-3-11eb11d4-3be6-4adf-97b7-83f55437b38d', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Successfully joined group with generation Generation{generationId=1, memberId='consumer-groupST-2-faba98d1-073f-4166-aaee-518537b566df', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Finished assignment for group at generation 1: {consumer-groupST-3-11eb11d4-3be6-4adf-97b7-83f55437b38d=Assignment(partitions=[second_topic-2]), consumer-groupST-2-faba98d1-073f-4166-aaee-518537b566df=Assignment(partitions=[second_topic-1]), consumer-groupST-1-ada12034-52f3-41a8-9b62-d3efd648bc32=Assignment(partitions=[second_topic-0])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Successfully synced group in generation Generation{generationId=1, memberId='consumer-groupST-2-faba98d1-073f-4166-aaee-518537b566df', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Successfully synced group in generation Generation{generationId=1, memberId='consumer-groupST-3-11eb11d4-3be6-4adf-97b7-83f55437b38d', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Successfully synced group in generation Generation{generationId=1, memberId='consumer-groupST-1-ada12034-52f3-41a8-9b62-d3efd648bc32', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-0])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Adding newly assigned partitions: second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Adding newly assigned partitions: second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Found no committed offset for partition second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Found no committed offset for partition second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Found no committed offset for partition second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Found no committed offset for partition second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Found no committed offset for partition second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Found no committed offset for partition second_topic-1
INFO  SubscriptionState - [Consumer clientId=consumer-groupST-3, groupId=groupST] Resetting offset for partition second_topic-2 to position FetchPosition{offset=120, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}.
INFO  SubscriptionState - [Consumer clientId=consumer-groupST-1, groupId=groupST] Resetting offset for partition second_topic-0 to position FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}.
INFO  SubscriptionState - [Consumer clientId=consumer-groupST-2, groupId=groupST] Resetting offset for partition second_topic-1 to position FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}.
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-0]
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-1]
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-2]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Revoke previously assigned partitions second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Revoke previously assigned partitions second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-0]
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-2]
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-1]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Member consumer-groupST-1-ada12034-52f3-41a8-9b62-d3efd648bc32 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Member consumer-groupST-3-11eb11d4-3be6-4adf-97b7-83f55437b38d sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Member consumer-groupST-2-faba98d1-073f-4166-aaee-518537b566df sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-1, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-2, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-3, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-3 unregistered
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-2 unregistered
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-1 unregistered
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
INFO  PlaygroundFrameworksSpringbootApplication - Starting PlaygroundFrameworksSpringbootApplication using Java 21.0.1 with PID 15377 (/media/kkpa/Data/Developer/github/java-projects/JavaPlayground/playground-frameworks-springboot/build/classes/java/main started by kkpa in /media/kkpa/Data/Developer/github/java-projects/JavaPlayground)
INFO  PlaygroundFrameworksSpringbootApplication - No active profile set, falling back to 1 default profile: "default"
INFO  TomcatWebServer - Tomcat initialized with port 8888 (http)
INFO  Http11NioProtocol - Initializing ProtocolHandler ["http-nio-8888"]
INFO  StandardService - Starting service [Tomcat]
INFO  StandardEngine - Starting Servlet engine: [Apache Tomcat/10.1.16]
INFO  [/pgf/api] - Initializing Spring embedded WebApplicationContext
INFO  ServletWebServerApplicationContext - Root WebApplicationContext: initialization completed in 1257 ms
INFO  ProducerConfig - ProducerConfig values: 
	acks = -1
	auto.include.jmx.reporter = true
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.StringSerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.adaptive.partitioning.enable = true
	partitioner.availability.timeout.ms = 0
	partitioner.class = null
	partitioner.ignore.keys = false
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 3
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.StringSerializer

INFO  KafkaProducer - [Producer clientId=producer-1] Instantiated an idempotent producer.
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709845013916
INFO  Metadata - [Producer clientId=producer-1] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  TransactionManager - [Producer clientId=producer-1] ProducerId set to 3019 with epoch 0
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:120 Timestamp:1709845014291
INFO  ProducerNoKeysStickyP - Message sent! Partition:2  Offset:121 Timestamp:1709845015306
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[0:0]]  Consumer[Partition:1 Offset:96 Timestamp:1709845016313]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[0:1]]  Consumer[Partition:0 Offset:155 Timestamp:1709845017315]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[0:2]]  Consumer[Partition:2 Offset:122 Timestamp:1709845018315]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[0:3]]  Consumer[Partition:0 Offset:156 Timestamp:1709845019316]
INFO  ProducerWithKeys - Message-KEY! [Key:id_0-Value:[1:0]]  Consumer[Partition:1 Offset:97 Timestamp:1709845020317]
INFO  ProducerWithKeys - Message-KEY! [Key:id_1-Value:[1:1]]  Consumer[Partition:0 Offset:157 Timestamp:1709845021318]
INFO  ProducerWithKeys - Message-KEY! [Key:id_2-Value:[1:2]]  Consumer[Partition:2 Offset:123 Timestamp:1709845022318]
INFO  ProducerWithKeys - Message-KEY! [Key:id_3-Value:[1:3]]  Consumer[Partition:0 Offset:158 Timestamp:1709845023319]
INFO  Http11NioProtocol - Starting ProtocolHandler ["http-nio-8888"]
INFO  TomcatWebServer - Tomcat started on port 8888 (http) with context path '/pgf/api'
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-1
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709845024754
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-1, groupId=groupST] Subscribed to topic(s): second_topic
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709845024771
INFO  Metadata - [Consumer clientId=consumer-groupST-1, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-2, groupId=groupST] Subscribed to topic(s): second_topic
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerConfig - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.include.jmx.reporter = true
	auto.offset.reset = latest
	bootstrap.servers = [localhost:9092]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = consumer-groupST-3
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = groupST
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.StringDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = GSSAPI
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.StringDeserializer

INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] (Re-)joining group
INFO  Metadata - [Consumer clientId=consumer-groupST-2, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] (Re-)joining group
INFO  AppInfoParser - Kafka version: 3.6.0
INFO  AppInfoParser - Kafka commitId: 60e845626d8a465a
INFO  AppInfoParser - Kafka startTimeMs: 1709845024779
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-3, groupId=groupST] Subscribed to topic(s): second_topic
INFO  Metadata - [Consumer clientId=consumer-groupST-3, groupId=groupST] Cluster ID: Some(5L6g3nShT-eMCtK--X86sw)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Discovered group coordinator localhost:9092 (id: 2147483646 rack: null)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-2-d684de0e-b1ac-4037-9a8f-4ca7f18f5600
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-3-b19367f3-8b4d-4302-bc39-b38b954c777a
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: need to re-join with the given member-id: consumer-groupST-1-1ee0f24d-2c9d-4999-a55a-2ead92abf39b
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  PlaygroundFrameworksSpringbootApplication - Started PlaygroundFrameworksSpringbootApplication in 14.035 seconds (process running for 14.848)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: rebalance failed due to 'The group member needs to have a valid member id before actually entering a consumer group.' (MemberIdRequiredException)
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] (Re-)joining group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Successfully joined group with generation Generation{generationId=3, memberId='consumer-groupST-3-b19367f3-8b4d-4302-bc39-b38b954c777a', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Successfully joined group with generation Generation{generationId=3, memberId='consumer-groupST-1-1ee0f24d-2c9d-4999-a55a-2ead92abf39b', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Successfully joined group with generation Generation{generationId=3, memberId='consumer-groupST-2-d684de0e-b1ac-4037-9a8f-4ca7f18f5600', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Finished assignment for group at generation 3: {consumer-groupST-3-b19367f3-8b4d-4302-bc39-b38b954c777a=Assignment(partitions=[second_topic-2]), consumer-groupST-2-d684de0e-b1ac-4037-9a8f-4ca7f18f5600=Assignment(partitions=[second_topic-1]), consumer-groupST-1-1ee0f24d-2c9d-4999-a55a-2ead92abf39b=Assignment(partitions=[second_topic-0])}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Successfully synced group in generation Generation{generationId=3, memberId='consumer-groupST-1-1ee0f24d-2c9d-4999-a55a-2ead92abf39b', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Successfully synced group in generation Generation{generationId=3, memberId='consumer-groupST-3-b19367f3-8b4d-4302-bc39-b38b954c777a', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Successfully synced group in generation Generation{generationId=3, memberId='consumer-groupST-2-d684de0e-b1ac-4037-9a8f-4ca7f18f5600', protocol='range'}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-1])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-2])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Notifying assignor about the new Assignment(partitions=[second_topic-0])
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Adding newly assigned partitions: second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Adding newly assigned partitions: second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Adding newly assigned partitions: second_topic-2
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Setting offset for partition second_topic-1 to the committed offset FetchPosition{offset=96, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Setting offset for partition second_topic-2 to the committed offset FetchPosition{offset=120, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Setting offset for partition second_topic-0 to the committed offset FetchPosition{offset=155, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[localhost:9092 (id: 1 rack: null)], epoch=10}}
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-1]
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-2]
INFO  KafkaMessageListenerContainer - groupST: partitions assigned: [second_topic-0]
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:null_Partition:2_Offset:120] - Value:Sticky Value:0 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:96] - Value:[0:0] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_1_Partition:0_Offset:155] - Value:[0:1] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_3_Partition:0_Offset:156] - Value:[0:3] 
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:null_Partition:2_Offset:121] - Value:Sticky Value:1 
INFO  SecondTopicConsumerGroup - Consumer 1 [Key:id_0_Partition:1_Offset:97] - Value:[1:0] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_1_Partition:0_Offset:157] - Value:[1:1] 
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:id_2_Partition:2_Offset:122] - Value:[0:2] 
INFO  SecondTopicConsumerGroup - Consumer 3 [Key:id_3_Partition:0_Offset:158] - Value:[1:3] 
INFO  SecondTopicConsumerGroup - Consumer 2 [Key:id_2_Partition:2_Offset:123] - Value:[1:2] 
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Revoke previously assigned partitions second_topic-0
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Revoke previously assigned partitions second_topic-1
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Revoke previously assigned partitions second_topic-2
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-0]
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-1]
INFO  KafkaMessageListenerContainer - groupST: partitions revoked: [second_topic-2]
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Member consumer-groupST-3-b19367f3-8b4d-4302-bc39-b38b954c777a sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Member consumer-groupST-1-1ee0f24d-2c9d-4999-a55a-2ead92abf39b sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Member consumer-groupST-2-d684de0e-b1ac-4037-9a8f-4ca7f18f5600 sending LeaveGroup request to coordinator localhost:9092 (id: 2147483646 rack: null) due to the consumer unsubscribed from all topics
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-1, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-2, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  DefaultKafkaConsumerFactory$ExtendedKafkaConsumer - [Consumer clientId=consumer-groupST-3, groupId=groupST] Unsubscribed all topics or patterns and assigned partitions
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-3, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Resetting generation and member id due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-1, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  ConsumerCoordinator - [Consumer clientId=consumer-groupST-2, groupId=groupST] Request joining group due to: consumer pro-actively leaving the group
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-3 unregistered
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-2 unregistered
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  AppInfoParser - App info kafka.consumer for consumer-groupST-1 unregistered
INFO  KafkaMessageListenerContainer - groupST: Consumer stopped
INFO  KafkaProducer - [Producer clientId=producer-1] Closing the Kafka producer with timeoutMillis = 30000 ms.
INFO  Metrics - Metrics scheduler closed
INFO  Metrics - Closing reporter org.apache.kafka.common.metrics.JmxReporter
INFO  Metrics - Metrics reporters closed
INFO  AppInfoParser - App info kafka.producer for producer-1 unregistered
